{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e28dd0",
   "metadata": {},
   "source": [
    "# Simple Model - Distance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d905d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_utils import  *\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_json(\"../data/train_data_embedded.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc7b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train a simple lightgbm model on the cross validation set\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "rel_features = ['metric_name', 'user_prompt', 'response', 'system_prompt']\n",
    "#create all possible 2 combinations of rel_features\n",
    "ed_rel_features = []\n",
    "for i in range(len(rel_features)):\n",
    "    for j in range(i + 1, len(rel_features)):\n",
    "        ed_rel_features.append((rel_features[i], rel_features[j]))\n",
    "ed_only_config = {'euclidean_distances': ed_rel_features,}\n",
    "\n",
    "\n",
    "fe = FeatureEngineer(train_data, ed_only_config)\n",
    "df_features = fe.create_features()\n",
    "df_features.set_index('index')\n",
    "\n",
    "data_gen = DataGenerator(df_features, test_fraction=0.2, fold_k=4, metric_column='main_metric')\n",
    "dataset = data_gen.generate_splits()\n",
    "cross_val_set = dataset['cv_splits']\n",
    "cross_val_set[0][0]['score']\n",
    "\n",
    "\n",
    "\n",
    "rmse_scores = []\n",
    "N_SPLITS = 4\n",
    "# This loop runs 4 times\n",
    "for fold in range(N_SPLITS):\n",
    "    \n",
    "    print(f\"--- FOLD {fold+1}/{N_SPLITS} ---\")\n",
    "    \n",
    "    # 1. Create the training and validation sets for this fold\n",
    "    X_train, y_train = prepare_final(cross_val_set[fold][0])\n",
    "    X_val, y_val = prepare_final(cross_val_set[fold][1])\n",
    "\n",
    "    # 2. Initialize your model(train_index, val_index) in enumerate(skf.split(X, y_metric)):\n",
    "    # (You can also test your Classifier hack here)\n",
    "    model = lgb.LGBMRegressor(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # 3. Train the model\n",
    "    # We use early_stopping to prevent overfitting inside the fold\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric='rmse',\n",
    "        callbacks=[lgb.early_stopping(100, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    # 4. Get predictions and calculate the score for this fold\n",
    "    preds = model.predict(X_val)\n",
    "    fold_rmse = root_mean_squared_error(y_val, preds)\n",
    "\n",
    "    print(f\"Fold {fold+1} RMSE: {fold_rmse}\")\n",
    "    rmse_scores.append(fold_rmse)\n",
    "\n",
    "# 5. Get your final, reliable score\n",
    "print(\"\\n--- Cross-Validation Summary ---\")\n",
    "print(f\"All RMSE Scores: {rmse_scores}\")\n",
    "print(f\"Average RMSE: {np.mean(rmse_scores)}\")\n",
    "print(f\"Std Dev of RMSE: {np.std(rmse_scores)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbfdc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "scores=[]\n",
    "N_SPLITS = 4\n",
    "\n",
    "\n",
    "# This loop runs 4 times\n",
    "for fold in range(N_SPLITS):\n",
    "    \n",
    "    print(f\"--- FOLD {fold+1}/{N_SPLITS} ---\")\n",
    "    \n",
    "    # 1. Create the training and validation sets for this fold\n",
    "    X_train, y_train = prepare_final(cross_val_set[fold][0])\n",
    "    X_val, y_val = prepare_final(cross_val_set[fold][1])\n",
    "\n",
    "    # 2. Initialize your model(train_index, val_index) in enumerate(skf.split(X, y_metric)):\n",
    "    # (You can also test your Classifier hack here)\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # 3. Train the model\n",
    "    # We use early_stopping to prevent overfitting inside the fold\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric='logloss',\n",
    "        callbacks=[lgb.early_stopping(100, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    # 4. Get predictions and calculate the score for this fold\n",
    "    preds = model.predict(X_val)\n",
    "    fold_accuracy = accuracy_score(y_val, preds)\n",
    "    fold_precision = precision_score(y_val, preds, average='weighted', zero_division=0)\n",
    "    fold_recall = recall_score(y_val, preds, average='weighted', zero_division=0)\n",
    "    fold_f1 = f1_score(y_val, preds, average='weighted', zero_division=0)\n",
    "\n",
    "    print(f\"Fold {fold+1} Accuracy: {fold_accuracy}\")\n",
    "    print(f\"Fold {fold+1} Precision: {fold_precision}\")\n",
    "    print(f\"Fold {fold+1} Recall: {fold_recall}\")\n",
    "    print(f\"Fold {fold+1} F1 Score: {fold_f1}\")\n",
    "\n",
    "    scores.append({\n",
    "        \"accuracy\": fold_accuracy,\n",
    "        \"precision\": fold_precision,\n",
    "        \"recall\": fold_recall,\n",
    "        \"f1\": fold_f1\n",
    "    })\n",
    "f1_scores = [score['f1'] for score in scores]\n",
    "accuracy_scores = [score['accuracy'] for score in scores]\n",
    "precision_scores = [score['precision'] for score in scores]\n",
    "recall_scores = [score['recall'] for score in scores]\n",
    "\n",
    "#get average of all metrics\n",
    "print(\"\\n--- Cross-Validation Summary ---\")\n",
    "print(f\"Average Accuracy: {np.mean(accuracy_scores)}\")\n",
    "print(f\"Average Precision: {np.mean(precision_scores)}\")\n",
    "print(f\"Average Recall: {np.mean(recall_scores)}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486de2d",
   "metadata": {},
   "source": [
    "# Model 2 - Difference Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c205363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>user_prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>system_prompt</th>\n",
       "      <th>score</th>\n",
       "      <th>main_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[-0.100472755730152, 0.014361751265823001, -0....</td>\n",
       "      <td>[-0.051010597497224, -0.054428562521934, 0.016...</td>\n",
       "      <td>[-0.046048831194639005, 0.022551368921995003, ...</td>\n",
       "      <td>[-0.06521516293287201, 0.030161105096340003, 0...</td>\n",
       "      <td>10</td>\n",
       "      <td>rejection_rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[-0.11168923228979101, 0.022300960496068, 0.02...</td>\n",
       "      <td>[0.027696760371327, -0.052469231188297, 0.0128...</td>\n",
       "      <td>[0.018323564901947, 0.023317318409681, 0.03503...</td>\n",
       "      <td>[-0.06438597291707901, -0.0030478346161540004,...</td>\n",
       "      <td>10</td>\n",
       "      <td>exaggerated_safety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[-0.131887972354888, 0.000410302251111, 0.0110...</td>\n",
       "      <td>[-0.024716671556234002, 0.021793110296130003, ...</td>\n",
       "      <td>[-0.037564639002084003, 0.011249059811234, -0....</td>\n",
       "      <td>[-0.198776066303253, -0.008929803967475001, 0....</td>\n",
       "      <td>10</td>\n",
       "      <td>bias_detection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[-0.126572892069816, -0.006100242491811, -0.00...</td>\n",
       "      <td>[0.007488088216632001, 0.031233424320816, 0.00...</td>\n",
       "      <td>[0.011075009591877, 0.07769121974706601, 0.021...</td>\n",
       "      <td>[-0.198776066303253, -0.008929803967475001, 0....</td>\n",
       "      <td>10</td>\n",
       "      <td>confidence_agreement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[-0.174830943346023, -0.036134198307991, 0.046...</td>\n",
       "      <td>[-0.027865169569849003, -0.014575573615729, -0...</td>\n",
       "      <td>[-0.017473634332418, -0.034948986023664, -0.00...</td>\n",
       "      <td>[-0.098706573247909, -0.001185112050734, 0.027...</td>\n",
       "      <td>9</td>\n",
       "      <td>cultural_sensitivity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                        metric_name  \\\n",
       "0      0  [-0.100472755730152, 0.014361751265823001, -0....   \n",
       "1      1  [-0.11168923228979101, 0.022300960496068, 0.02...   \n",
       "2      2  [-0.131887972354888, 0.000410302251111, 0.0110...   \n",
       "3      3  [-0.126572892069816, -0.006100242491811, -0.00...   \n",
       "4      4  [-0.174830943346023, -0.036134198307991, 0.046...   \n",
       "\n",
       "                                         user_prompt  \\\n",
       "0  [-0.051010597497224, -0.054428562521934, 0.016...   \n",
       "1  [0.027696760371327, -0.052469231188297, 0.0128...   \n",
       "2  [-0.024716671556234002, 0.021793110296130003, ...   \n",
       "3  [0.007488088216632001, 0.031233424320816, 0.00...   \n",
       "4  [-0.027865169569849003, -0.014575573615729, -0...   \n",
       "\n",
       "                                            response  \\\n",
       "0  [-0.046048831194639005, 0.022551368921995003, ...   \n",
       "1  [0.018323564901947, 0.023317318409681, 0.03503...   \n",
       "2  [-0.037564639002084003, 0.011249059811234, -0....   \n",
       "3  [0.011075009591877, 0.07769121974706601, 0.021...   \n",
       "4  [-0.017473634332418, -0.034948986023664, -0.00...   \n",
       "\n",
       "                                       system_prompt  score  \\\n",
       "0  [-0.06521516293287201, 0.030161105096340003, 0...     10   \n",
       "1  [-0.06438597291707901, -0.0030478346161540004,...     10   \n",
       "2  [-0.198776066303253, -0.008929803967475001, 0....     10   \n",
       "3  [-0.198776066303253, -0.008929803967475001, 0....     10   \n",
       "4  [-0.098706573247909, -0.001185112050734, 0.027...      9   \n",
       "\n",
       "            main_metric  \n",
       "0        rejection_rate  \n",
       "1    exaggerated_safety  \n",
       "2        bias_detection  \n",
       "3  confidence_agreement  \n",
       "4  cultural_sensitivity  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "train_data = pd.read_json(\"../data/train_data_embedded.json\", lines=True)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1395e335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating feature: difference_vectors--response-user_prompt\n",
      "Created feature: difference_vectors--response-user_prompt\n",
      "Creating feature: difference_vectors--response-system_prompt\n",
      "Created feature: difference_vectors--response-system_prompt\n",
      "Creating feature: difference_vectors--response-metric_name\n",
      "Created feature: difference_vectors--response-metric_name\n",
      "Creating feature: difference_vectors--user_prompt-metric_name\n",
      "Created feature: difference_vectors--user_prompt-metric_name\n",
      "Creating feature: euclidean_distances--metric_name-user_prompt\n",
      "Created feature: euclidean_distances--metric_name-user_prompt\n",
      "Creating feature: euclidean_distances--metric_name-response\n",
      "Created feature: euclidean_distances--metric_name-response\n",
      "Creating feature: euclidean_distances--metric_name-system_prompt\n",
      "Created feature: euclidean_distances--metric_name-system_prompt\n",
      "Creating feature: euclidean_distances--user_prompt-response\n",
      "Created feature: euclidean_distances--user_prompt-response\n",
      "Creating feature: euclidean_distances--user_prompt-system_prompt\n",
      "Created feature: euclidean_distances--user_prompt-system_prompt\n",
      "Creating feature: euclidean_distances--response-system_prompt\n",
      "Created feature: euclidean_distances--response-system_prompt\n"
     ]
    }
   ],
   "source": [
    "import project_utils\n",
    "import importlib\n",
    "importlib.reload(project_utils)\n",
    "\n",
    "\n",
    "rel_features = ['metric_name', 'user_prompt', 'response', 'system_prompt']\n",
    "#create all possible 2 combinations of rel_features\n",
    "ed_rel_features = []\n",
    "for i in range(len(rel_features)):\n",
    "    for j in range(i + 1, len(rel_features)):\n",
    "        ed_rel_features.append((rel_features[i], rel_features[j]))\n",
    "ed_only_config = {'euclidean_distances': ed_rel_features,}\n",
    "\n",
    "diff_vector_config = {'difference_vectors': [('response', 'user_prompt'), ('response', 'system_prompt'),('response', 'metric_name'),('user_prompt', 'metric_name')], 'euclidean_distances': ed_rel_features}\n",
    "fe = project_utils.FeatureEngineer(train_data, diff_vector_config)\n",
    "df_diff_vector = fe.create_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e389d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = project_utils.DataGenerator(df_diff_vector, test_fraction=0.2, fold_k=4, metric_column='main_metric')\n",
    "df_diff = data_gen.generate_splits()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9eff9a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "--- FOLD 1/4 ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.203447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 784890\n",
      "[LightGBM] [Info] Number of data points in the train set: 2999, number of used features: 3078\n",
      "[LightGBM] [Info] Start training from score 9.120040\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's rmse: 0.93012\tvalid_0's l2: 0.865122\n",
      "Fold 1 RMSE: 0.9301195184883791\n",
      "--- FOLD 2/4 ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.215856 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 784890\n",
      "[LightGBM] [Info] Number of data points in the train set: 2999, number of used features: 3078\n",
      "[LightGBM] [Info] Start training from score 9.120040\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's rmse: 0.822506\tvalid_0's l2: 0.676515\n",
      "Fold 2 RMSE: 0.822505500598343\n",
      "--- FOLD 3/4 ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.215465 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 784890\n",
      "[LightGBM] [Info] Number of data points in the train set: 2999, number of used features: 3078\n",
      "[LightGBM] [Info] Start training from score 9.123041\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid_0's rmse: 0.95187\tvalid_0's l2: 0.906057\n",
      "Fold 3 RMSE: 0.9518702753246426\n",
      "--- FOLD 4/4 ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.222771 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 784890\n",
      "[LightGBM] [Info] Number of data points in the train set: 3000, number of used features: 3078\n",
      "[LightGBM] [Info] Start training from score 9.112000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's rmse: 0.896324\tvalid_0's l2: 0.803398\n",
      "Fold 4 RMSE: 0.8963244668326075\n",
      "\n",
      "--- Cross-Validation Summary ---\n",
      "All RMSE Scores: [0.9301195184883791, 0.822505500598343, 0.9518702753246426, 0.8963244668326075]\n",
      "Average RMSE: 0.9002049403109931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'average_rmse': np.float64(0.9002049403109931),\n",
       " 'fold_metrics': [0.9301195184883791,\n",
       "  0.822505500598343,\n",
       "  0.9518702753246426,\n",
       "  0.8963244668326075]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_utils.LBGMCrossValidate(df_diff,fold_k =4,model_type='regressor',random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25daae6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
